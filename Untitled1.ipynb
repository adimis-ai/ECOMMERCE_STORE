{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw8/b3GeZliLUKOYupVjrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adimis-ai/ECOMMERCE_STORE/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem statement\n",
        "\n",
        "Loop monitors several restaurants in the US and needs to monitor if the store is online or not. All restaurants are supposed to be online during their business hours. Due to some unknown reasons, a store might go inactive for a few hours. Restaurant owners want to get a report of the how often this happened in the past.   \n",
        "\n",
        "We want to build backend APIs that will help restaurant owners achieve this goal.\n",
        "\n",
        "We will provide the following data sources which contain all the data that is required to achieve this purpose.\n",
        "\n",
        "## Data sources\n",
        "\n",
        "We will have 3 sources of data\n",
        "\n",
        "1. We poll every store roughly every hour and have data about whether the store was active or not in a CSV.  The CSV has 3 columns (`store_id, timestamp_utc, status`) where status is active or inactive.  All timestamps are in **UTC**\n",
        "    - Data can be found in CSV format below:\n",
        "    **/home/adimis/Desktop/backend/store_monitoring_project/database/store status.csv**\n",
        "    ```csv\n",
        "    store_id,status,timestamp_utc\n",
        "    8419537941919820732,active,2023-01-22 12:09:39.388884 UTC\n",
        "    54515546588432327,active,2023-01-24 09:06:42.605777 UTC\n",
        "    8377465688456570187,active,2023-01-24 09:07:26.441407 UTC\n",
        "    5955337179846162144,active,2023-01-24 09:08:07.634899 UTC\n",
        "    1169347689335808384,active,2023-01-24 09:08:18.436854 UTC\n",
        "    3739729523266121270,active,2023-01-24 09:08:23.138922 UTC\n",
        "    4430370444829587327,active,2023-01-24 09:09:37.456563 UTC\n",
        "    ```\n",
        "\n",
        "2. We have the business hours of all the stores - schema of this data is `store_id, dayOfWeek(0=Monday, 6=Sunday), start_time_local, end_time_local`\n",
        "    - These times are in the **local time zone**\n",
        "    - If data is missing for a store, assume it is open 24*7\n",
        "    - Data can be found in CSV format below:\n",
        "    **/home/adimis/Desktop/backend/store_monitoring_project/database/Menu hours.csv**\n",
        "    ```csv\n",
        "    store_id,day,start_time_local,end_time_local\n",
        "    1481966498820158979,4,00:00:00,00:10:00\n",
        "    1481966498820158979,2,00:00:00,00:10:00\n",
        "    1481966498820158979,0,00:00:00,00:10:00\n",
        "    1481966498820158979,1,00:00:00,00:10:00\n",
        "    1481966498820158979,5,00:00:00,00:10:00\n",
        "    1481966498820158979,3,00:00:00,00:10:00\n",
        "    1481966498820158979,6,00:00:00,00:10:00\n",
        "    579100056021594375,5,00:00:00,00:10:00\n",
        "    579100056021594375,1,00:00:00,00:10:00\n",
        "    579100056021594375,3,00:00:00,00:10:00\n",
        "    579100056021594375,4,00:00:00,00:10:00\n",
        "    579100056021594375,2,00:00:00,00:10:00\n",
        "    579100056021594375,0,00:00:00,00:10:00\n",
        "    579100056021594375,6,00:00:00,00:10:00\n",
        "    ```\n",
        "\n",
        "3. Timezone for the stores - schema is `store_id, timezone_str`\n",
        "    - If data is missing for a store, assume it is America/Chicago\n",
        "    - This is used so that data sources 1 and 2 can be compared against each other.\n",
        "    - Data can be found in CSV format below\n",
        "    **/home/adimis/Desktop/backend/store_monitoring_project/database/bq-results-20230125-202210-1674678181880.csv**\n",
        "    ```csv\n",
        "    store_id,timezone_str\n",
        "    8139926242460185114,Asia/Beirut\n",
        "    5415949628544298339,America/Boise\n",
        "    3408529570017053440,America/Denver\n",
        "    9055649751952768824,America/Denver\n",
        "    4428372089193592098,America/Denver\n",
        "    2689959411535120475,America/Denver\n",
        "    8297996490922435741,America/Denver\n",
        "    1050565545391667097,America/Denver\n",
        "    3483930781272060942,America/Denver\n",
        "    1740222068509982431,America/Denver\n",
        "    2859012985063828777,America/Denver\n",
        "    ```\n",
        "\n",
        "## System requirement:\n",
        "- Do not assume that this data is static and precompute the answers as this data will keep getting updated every hour.\n",
        "- You need to store these CSVs into a relevant database and make API calls to get the data.\n",
        "\n",
        "## Data output requirement:\n",
        "We want to output a report to the user that has the following schema\n",
        "`store_id, uptime_last_hour(in minutes), uptime_last_day(in hours), update_last_week(in hours), downtime_last_hour(in minutes), downtime_last_day(in hours), downtime_last_week(in hours)`\n",
        "\n",
        "1. Uptime and downtime should only include observations within business hours.\n",
        "2. You need to extrapolate uptime and downtime based on the periodic polls we have ingested, to the entire time interval.\n",
        "    1. eg, business hours for a store are 9 AM to 12 PM on Monday\n",
        "        1. we only have 2 observations for this store on a particular date (Monday) in our data at 10:14 AM and 11:15 AM\n",
        "        2. we need to fill the entire business hours interval with uptime and downtime from these 2 observations based on some sane interpolation logic\n",
        "\n",
        "Note: The data we have given is a static data set, so you can hard code the current timestamp to be the max timestamp among all the observations in the first CSV.  \n",
        "\n",
        "## API requirement:\n",
        "1. You need two APIs\n",
        "    1. /trigger_report endpoint that will trigger report generation from the data provided (stored in DB)\n",
        "        1. No input\n",
        "        2. Output - report_id (random string)\n",
        "        3. report_id will be used for polling the status of report completion\n",
        "    2. /get_report endpoint that will return the status of the report or the csv\n",
        "        1. Input - report_id\n",
        "        2. Output\n",
        "            - if report generation is not complete, return “Running” as the output\n",
        "            - if report generation is complete, return “Complete” along with the CSV file with the schema described above.\n",
        "\n",
        "## Considerations/Evaluation criteria\n",
        "\n",
        "1. The code should be well structured, handling corner cases, with good type systems.\n",
        "2. The functionality should be correct for trigger + poll architecture, database reads and CSV output.\n",
        "3. The logic for computing the hours overlap and uptime/downtime should be well documented and easy to read/understand.\n",
        "4. The code should be as optimized as people and run within a reasonable amount of time."
      ],
      "metadata": {
        "id": "9eJKXvdwW49r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQsy4KTFWag0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import numpy as np\n",
        "\n",
        "# File paths\n",
        "store_status_path = '/content/gdrive/MyDrive/store status.csv'\n",
        "menu_hours_path = '/content/gdrive/MyDrive/Menu hours.csv'\n",
        "timezone_path = '/content/gdrive/MyDrive/bq-results-20230125-202210-1674678181880.csv'\n",
        "\n",
        "# Load CSV files into DataFrames\n",
        "store_status_df = pd.read_csv(store_status_path, parse_dates=['timestamp_utc'])\n",
        "menu_hours_df = pd.read_csv(menu_hours_path)\n",
        "timezone_df = pd.read_csv(timezone_path)\n",
        "\n",
        "print(\"\\nstore_status_df:\\n\",store_status_df.head(5))\n",
        "print(\"\\nstore_status_df:\\n\",menu_hours_df.head(5))\n",
        "print(\"\\nstore_status_df:\\n\",timezone_df.head(5))"
      ],
      "metadata": {
        "id": "2IxMncZMee8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(store_status_df, menu_hours_df, timezone_df):\n",
        "  # Step 1: Preprocess the data\n",
        "  store_status_df['timestamp_utc'] = pd.to_datetime(store_status_df['timestamp_utc'])\n",
        "  menu_hours_df['start_time_local'] = pd.to_datetime(menu_hours_df['start_time_local'], format='%H:%M:%S').dt.time\n",
        "  menu_hours_df['end_time_local'] = pd.to_datetime(menu_hours_df['end_time_local'], format='%H:%M:%S').dt.time\n",
        "\n",
        "  # Convert timezone_str to pytz timezone objects\n",
        "  timezone_df['timezone_obj'] = timezone_df['timezone_str'].apply(pytz.timezone)\n",
        "\n",
        "  # Step 2: Join the DataFrames\n",
        "  merged_df = store_status_df.merge(menu_hours_df, on='store_id', how='left')\n",
        "  merged_df = merged_df.merge(timezone_df[['store_id', 'timezone_obj']], on='store_id', how='left')\n",
        "\n",
        "  # Replace NaN values in 'timezone_obj' column with default timezone\n",
        "  default_timezone = pytz.timezone('America/Chicago')\n",
        "  merged_df['timezone_obj'] = merged_df['timezone_obj'].fillna(default_timezone)\n",
        "\n",
        "  # Function to convert timestamp_utc to local time\n",
        "  def convert_to_local_time(row):\n",
        "      try:\n",
        "          local_time = row['timestamp_utc'].astimezone(row['timezone_obj'])\n",
        "          return local_time\n",
        "      except:\n",
        "          return np.nan\n",
        "\n",
        "  # Apply the function to each row to get the 'timestamp_local' column\n",
        "  merged_df['timestamp_local'] = merged_df.apply(convert_to_local_time, axis=1)\n",
        "\n",
        "  def convert_to_local_time(row):\n",
        "      try:\n",
        "          local_time = row['timestamp_utc'].astimezone(row['timezone_obj'])\n",
        "          return local_time\n",
        "      except:\n",
        "          return pd.NaT\n",
        "\n",
        "  def convert_timestamp_to_local(merged_df):\n",
        "      # Apply the function to each row to get the 'timestamp_local' column\n",
        "      merged_df['timestamp_local'] = merged_df.apply(convert_to_local_time, axis=1)\n",
        "      return merged_df\n",
        "\n",
        "  return convert_timestamp_to_local(merged_df)\n",
        "\n",
        "merged_df = preprocess_data(store_status_df, menu_hours_df, timezone_df)\n",
        "print(\"\\nmerged_df:\\n\",merged_df)"
      ],
      "metadata": {
        "id": "SzMGt9xc3bMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_uptime_and_downtime(merged_df):\n",
        "    # Group the data by store_id and day to calculate uptime and downtime\n",
        "    grouped_df = merged_df.groupby(['store_id', 'day'])\n",
        "\n",
        "    # Calculate the duration of uptime and downtime for each group\n",
        "    grouped_df['status_num'] = grouped_df['status'].apply(lambda x: 1 if x == 'active' else 0)\n",
        "    grouped_df['time_diff'] = grouped_df['timestamp_local'].diff().dt.total_seconds().fillna(0)\n",
        "    grouped_df['uptime'] = grouped_df['time_diff'] * grouped_df['status_num']\n",
        "    grouped_df['downtime'] = grouped_df['time_diff'] * (1 - grouped_df['status_num'])\n",
        "\n",
        "    # Sum the duration of uptime and downtime for each store and day\n",
        "    aggregated_df = grouped_df.groupby(['store_id', 'day']).agg({\n",
        "        'uptime': 'sum',\n",
        "        'downtime': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "aggregated_df = calculate_uptime_and_downtime(merged_df)\n",
        "print(\"\\naggregated_df:\\n\", aggregated_df)"
      ],
      "metadata": {
        "id": "jf6vFxAV0yEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extrapolate_data(aggregated_df):\n",
        "  return aggregated_df\n",
        "\n",
        "def generate_report(extrapolated_df):\n",
        "  return extrapolated_df\n",
        "\n",
        "def report_generation_driver():\n",
        "  merged_df = preprocess_data(store_status_df, menu_hours_df, timezone_df)\n",
        "  aggregated_df = calculate_uptime_and_downtime(merged_df)\n",
        "  extrapolated_df = extrapolate_data(aggregated_df)\n",
        "  return generate_report(extrapolated_df)"
      ],
      "metadata": {
        "id": "fYaVnSqk2UAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}